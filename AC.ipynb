{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cb8faf",
   "metadata": {},
   "source": [
    "MediaPipe is an open-source framework developed by Google that provides a comprehensive solution for building real-time perception pipelines for a wide range of applications, including object detection, facial recognition, hand tracking, pose estimation, and more. It offers pre-trained machine learning models, as well as tools and components for processing, analyzing, and interpreting various forms of media input, such as images, video streams, and sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0df0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "944f7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the coordinates of the points traversed by the finger \n",
    "bpoints = [deque(maxlen=1024)]\n",
    "gpoints = [deque(maxlen=1024)]\n",
    "rpoints = [deque(maxlen=1024)]\n",
    "ypoints = [deque(maxlen=1024)]\n",
    "\n",
    "\n",
    "# These indexes will be used to mark the points in particular arrays of specific colour\n",
    "blue_index = 0\n",
    "green_index = 0\n",
    "red_index = 0\n",
    "yellow_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "298de55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors in BGR format Blue, Green, Red, Yellow \n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255)]\n",
    "\n",
    "# Which color is selected\n",
    "colorIndex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6739c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paint window of 471 lenght, 636 breadth, 3 width , white color\n",
    "paintWindow = np.zeros((471,636,3)) + 255\n",
    "\n",
    "cv2.namedWindow('Paint', cv2.WINDOW_AUTOSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b775b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=mp_hand_gesture. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(mp_hand_gesture, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# provides utility functions for drawing landmarks and connections on images or video frames\u001b[39;00m\n\u001b[0;32m     11\u001b[0m mpDraw \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmp_hand_gesture\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mokay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthumbs up\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthumbs down\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall me\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrock\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlive long\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmile\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_api.py:191\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File format not supported: filepath=mp_hand_gesture. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(mp_hand_gesture, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "#Initialise Mediapipe\n",
    "\n",
    "# This module provides functionality for detecting and tracking hands in images or video streams.\n",
    "mpHands = mp.solutions.hands\n",
    "\n",
    "# max_num_hands=1 specifies that the model should detect at most one hand in the input\n",
    "# min_detection_confidence=0.7 means hand detections with a confidence score >= 0.7 will be considered valid.\n",
    "hands = mpHands.Hands(max_num_hands = 1, min_detection_confidence = 0.7)\n",
    "\n",
    "# provides utility functions for drawing landmarks and connections on images or video frames\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "model = load_model('mp_hand_gesture')\n",
    "labels = ['okay', 'peace', 'thumbs up', 'thumbs down', 'call me', 'stop', 'rock', 'live long', 'fist', 'smile']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9777b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret = True\n",
    "\n",
    "while ret:\n",
    "    # Used to capture a frame from a video\n",
    "    # read() returns tow values boolean ret = True when the frame is captured successfully\n",
    "    # frame contains the actual image data of the captured frame.\n",
    "    ret, frame = cap.read()\n",
    "    # x = heaight, y = width, c = no.of channels(1 if img is in gray scale, 3 if in RGB)\n",
    "    x, y, c = frame.shape\n",
    "    \n",
    "    \n",
    "    # Flip the along vertical axis (1 - along vertical axis, 0 means along horizontal axis)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # converting img from bgr format to rgb format\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Used to draw a rectangle on an img Frame, top left coordinates, bottom right coordinates, color of rectangle, border width in pixels   \n",
    "    frame = cv2.rectangle(frame, (40,1), (140,65), (0,0,0), 2)\n",
    "    frame = cv2.rectangle(frame, (160,1), (255,65), (255,0,0), 2)\n",
    "    frame = cv2.rectangle(frame, (275,1), (370,65), (0,255,0), 2)\n",
    "    frame = cv2.rectangle(frame, (390,1), (485,65), (0,0,255), 2)\n",
    "    frame = cv2.rectangle(frame, (505,1), (600,65), (0,255,255), 2)\n",
    "    \n",
    "    # Used to draw a text on an img Frame, starting location, font style, scale factor wrt default size, color of text, thickness, type of line\n",
    "    cv2.putText(frame, \"CLEAR\", (49, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"BLUE\", (185, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"GREEN\", (298, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"RED\", (420, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"YELLOW\", (520, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "     # Get hand landmark prediction\n",
    "     # result contains information such as the detected hand landmarks (e.g. keypoints representing fingertips, palm center)\n",
    "    result = hands.process(framergb)\n",
    "    \n",
    "    className = ''\n",
    "    \n",
    "    # if the result conatains hand\n",
    "    hand_present = result.multi_hand_landmarks\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        \n",
    "        for lm in result.multi_hand_landmarks[0].landmark:\n",
    "\n",
    "                # scaling the coordinates of landmarks acc to the paintWindow because the values are in [0,1]\n",
    "                lmx = int(lm.x * 640)\n",
    "                lmy = int(lm.y * 480)\n",
    "\n",
    "                landmarks.append([lmx, lmy])\n",
    "                \n",
    "        mpDraw.draw_landmarks(frame, result.multi_hand_landmarks[0], mpHands.HAND_CONNECTIONS)\n",
    "        \n",
    "        # Predict gesture in Hand Gesture Recognition project\n",
    "        prediction = model.predict([landmarks]) \n",
    "    \n",
    "        classID = np.argmax(prediction)\n",
    "        className = classNames[classID]\n",
    "\n",
    "        cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        fore_finger = (landmarks[8][0],landmarks[8][1])\n",
    "        thumb_finger = (landmarks[4][0],landmarks[4][1])\n",
    "        \n",
    "        distance = math.sqrt((fore_finger[0] - thumb_finger[0])**2 + (fore_finger[1] - thumb_finger[1])**2) \n",
    "        print(distance)\n",
    "        # do not write on the paintWindow when fore_finger and thumb are closer\n",
    "        if distance < 30:\n",
    "            bpoints.append(deque(maxlen=512))\n",
    "            blue_index += 1\n",
    "            gpoints.append(deque(maxlen=512))\n",
    "            green_index += 1\n",
    "            rpoints.append(deque(maxlen=512))\n",
    "            red_index += 1\n",
    "            ypoints.append(deque(maxlen=512))\n",
    "            yellow_index += 1\n",
    "           \n",
    "        # when the finger is in the button region\n",
    "        elif fore_finger[1] <= 65:\n",
    "            \n",
    "            # Clear button\n",
    "            if 40 <= fore_finger[0] <= 145:\n",
    "                bpoints = [deque(maxlen=512)]\n",
    "                gpoints = [deque(maxlen=512)]\n",
    "                rpoints = [deque(maxlen=512)]\n",
    "                ypoints = [deque(maxlen=512)]\n",
    "\n",
    "                blue_index = 0\n",
    "                green_index = 0\n",
    "                red_index = 0\n",
    "                yellow_index = 0\n",
    "\n",
    "                # clearing the paint window\n",
    "                paintWindow[67:,:,:] = 255\n",
    "                \n",
    "            elif 160 <= fore_finger[0] <= 255 or classID == 2: # Blue button\n",
    "                    colorIndex = 0 \n",
    "            elif 275 <= fore_finger[0] <= 370 or classID == 3: # Green button\n",
    "                    colorIndex = 1\n",
    "            elif 390 <= fore_finger[0] <= 485 or classID == 1: # Red button\n",
    "                    colorIndex = 2 \n",
    "            elif 505 <= fore_finger[0] <= 600 or classID == 0: # Yellow button\n",
    "                    colorIndex = 3  \n",
    "        else :\n",
    "            if colorIndex == 0:\n",
    "                bpoints[blue_index].appendleft(fore_finger)\n",
    "            elif colorIndex == 1:\n",
    "                gpoints[green_index].appendleft(fore_finger)\n",
    "            elif colorIndex == 2:\n",
    "                rpoints[red_index].appendleft(fore_finger)\n",
    "            elif colorIndex == 3:\n",
    "                ypoints[yellow_index].appendleft(fore_finger)\n",
    "    else :\n",
    "\n",
    "        bpoints.append(deque(maxlen=512))\n",
    "        blue_index += 1\n",
    "        gpoints.append(deque(maxlen=512))\n",
    "        green_index += 1\n",
    "        rpoints.append(deque(maxlen=512))\n",
    "        red_index += 1\n",
    "        ypoints.append(deque(maxlen=512))\n",
    "        yellow_index += 1\n",
    "            \n",
    "    points = [bpoints, gpoints, rpoints, ypoints]\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        for j in range(len(points[i])):\n",
    "            for k in range(1, len(points[i][j])):\n",
    "                if points[i][j][k - 1] is None or points[i][j][k] is None:\n",
    "                    continue\n",
    "                cv2.line(frame, points[i][j][k - 1], points[i][j][k], colors[i], 2)\n",
    "                cv2.line(paintWindow, points[i][j][k - 1], points[i][j][k], colors[i], 2)\n",
    "\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "    cv2.imshow(\"Paint\", paintWindow)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0f257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf153692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
